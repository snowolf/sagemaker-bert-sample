{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#整个过程是参考的tensorflow BERT官网实现https://github.com/google-research/bert，\n",
    "#以及一个外国工程师基于官网实现做的horovod分布式处理BERT finetuning的实现https://github.com/lambdal/bert\n",
    "\n",
    "#整个过程包括几个步骤：\n",
    "#1. 准备数据\n",
    "#2. 准备预训练好的BERT模型\n",
    "#3. 准备clone github的代码\n",
    "#4. 在本地测试基于BERT模型的finetuning的文本分类任务。\n",
    "#5. 将代码迁移到Sagemaker中，并利用horovod进行多机多卡的分布式训练\n",
    "\n",
    "#1. 利用官网提供的脚本来下载数据\n",
    "#步骤是：下载脚本；利用这个脚本下载数据；解压数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/archive/17b8dd0d724281ed7c3b2aeeda662b92809aadd5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip 17b8dd0d724281ed7c3b2aeeda662b92809aadd5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 60c2bdb54d156a41194446737ce03e2e-17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py --data_dir glue_data --tasks all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 准备预训练好的BERT模型\n",
    "#这里使用的是全部小写化的large模型：下载预训练的BERT模型并解压"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 准备clone github的代码\n",
    "#在sagemaker jupyter lab中对应的项目目录中创建source_code目录，\n",
    "#然后选择git按钮输入https://github.com/lambdal/bert.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkidr source_code\n",
    "cd source_code\n",
    "git clone https://github.com/lambdal/bert.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. 在本地测试基于BERT模型的finetuning的文本分类任务。\n",
    "#注意下面的一些参数： data_dir是需要使用的数据集，voca_file是需要用到的词典（来自上一步解压后的一个文件）\n",
    "#                  bert_config_file是bert的配置文件，init_checkpoint是预训练好的BERT模型。\n",
    "#                  output_dir是训练过程中的checkpoint保存的路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./source_code/bert/run_classifier.py \\\n",
    "  --task_name=MRPC \\\n",
    "  --do_train=true \\\n",
    "  --do_eval=true \\\n",
    "  --data_dir=./glue_data/MRPC \\\n",
    "  --vocab_file=./uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "  --bert_config_file=./uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "  --init_checkpoint=./uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "  --max_seq_length=128 \\\n",
    "  --train_batch_size=32 \\\n",
    "  --learning_rate=2e-5 \\\n",
    "  --num_train_epochs=1.0 \\\n",
    "  --output_dir=./mrpc_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. 将代码迁移到Sagemaker中，并利用horovod进行多机多卡的分布式训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.1 把预训练的包解压完的所有文件拷贝到source_code/bert目录下。\n",
    "#这样做的原因是在使用Sagemaker BYOS来训练的时候，训练脚本依赖的所有source code，配置文件，资源文件都需要和训练脚本在同一级的目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp uncased_L-12_H-768_A-12/* source_code/bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.2 把数据集上传到AWS的S3的桶（桶要提前建立）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 sync glue_data/MRPC s3://sagemaker-us-west-2-169088282855/bert-SM-TF-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.3 把训练脚本以及依赖的配置文件，资源文件（这里指的是预训练的模型文件）上传到S3的桶\n",
    "#其实不上传也没有问题，但是每次在开始训练前sagemaker都要重新对这些文件打包上传，影响训练启动速度。\n",
    "#因此最好的方式就是提前打包上传到S3.\n",
    "#把原来bert目录下的requirements.txt文件换个名字，否则Sagemaker会安装这个文件中的软件包（这里我们不需要安装任何软件包了）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv source_code/bert/requirements.txt source_code/bert/requirements.txt.copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "cd source_code/bert\n",
    "tar cvfz sourcedir.tar.gz *\n",
    "aws s3 cp sourcedir.tar.gz s3://sagemaker-us-west-2-169088282855/bert-hvd-tf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.4 调用Sagemaker的API开始finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里是使用horovod多机多卡训练，使用的是ml.p3.8xlarge（4个GPU卡），所以需要设置每个实例的worker数量为4\n",
    "#为了节省成本，这里使用spot实例来训练。\n",
    "#参数output_dir需要设置为/opt/ml/model，只有在这个路径下，sagemaker训练完才会把这个路径下的所有文件打包并上传。\n",
    "#参数data_dir需要设置为/opt/ml/input/data/training/，sagemaker file mode训练方式会把数据下载到这个目录，最后一级目录的名字（这里是training）需要和你的S3的channel的名字保持一致\n",
    "#参数do_train和do_eval需要设置为1，不能写True或者true，因为SM中使用hyperparameter对自己的脚本输入参数的时候，SM会把所有的参数都字符串化。\n",
    "#参数vocab_file，bert_config_file和init_checkpoint的路径前缀都是/opt/ml/code/，这个是因为sagemaker会把训练脚本以及所有依赖的源代码，配置文件，资源文件等都拷贝到这个目录。\n",
    "#entry_point设置为run_classifier_hvd.py进行分布式训练，这个脚本中已经写成了horovod集成tensorflow的方式。\n",
    "#source_dir参数指向指向打包代码以及依赖上传到S3的路径。\n",
    "#train_volume_size设置大一些（不一定像我下面设置的1TB这么大），否则模型在训练过程中保存checkpoint或者在训练完打包这些checkpoint上传S3的时候可能硬盘空间不够。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "train_instance_type = 'ml.p3.8xlarge'\n",
    "hvd_processes_per_host = 4\n",
    "distributions = {'mpi': {\n",
    "                    'enabled': True,\n",
    "                    'processes_per_host': hvd_processes_per_host,\n",
    "                    'custom_mpi_options': '-verbose --NCCL_DEBUG=INFO -x OMPI_MCA_btl_vader_single_copy_mechanism=none'\n",
    "                        }\n",
    "                }\n",
    "\n",
    "train_use_spot_instances = True\n",
    "train_max_run=432000\n",
    "train_max_wait = 432000 if train_use_spot_instances else None\n",
    "\n",
    "\n",
    "hyperparameters = {'output_dir': '/opt/ml/model', 'data_dir': '/opt/ml/input/data/training/',\n",
    "                   'do_train': 1, 'do_eval': 1,\n",
    "                   'vocab_file': '/opt/ml/code/vocab.txt', \n",
    "                   'bert_config_file': '/opt/ml/code/bert_config.json', \n",
    "                   'init_checkpoint': '/opt/ml/code/bert_model.ckpt',\n",
    "                   'num_train_epochs': 100.0, 'train_batch_size': 32, 'max_seq_length': 128, 'learning_rate': 2e-5, \n",
    "                   'task_name':'MRPC'}\n",
    "\n",
    "estimator = TensorFlow(entry_point='run_classifier_hvd.py',\n",
    "                       source_dir='s3://sagemaker-us-west-2-169088282855/bert-hvd-tf/sourcedir.tar.gz',\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=2,\n",
    "                       train_volume_size=1000,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-bert-sm-test',\n",
    "                       framework_version='1.14',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True,\n",
    "                       distributions=distributions,\n",
    "                       train_use_spot_instances=train_use_spot_instances,\n",
    "                       train_max_wait=train_max_wait,\n",
    "                       train_max_run=train_max_run\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3 = 's3://sagemaker-us-west-2-169088282855/bert-SM-TF-test'\n",
    "train_channel = 'training'\n",
    "\n",
    "inputs = {train_channel: train_s3}\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
